{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 8],\n",
       "        [8, 8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.mm(torch.tensor([[2,2], [2,2]]),\n",
    "         torch.tensor([[2,2], [2,2]])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.1-cp312-cp312-win_amd64.whl (198.5 MB)\n",
      "   ---------------------------------------- 0.0/198.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.6/198.5 MB 19.8 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 1.8/198.5 MB 23.1 MB/s eta 0:00:09\n",
      "    --------------------------------------- 3.0/198.5 MB 27.3 MB/s eta 0:00:08\n",
      "    --------------------------------------- 4.1/198.5 MB 26.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 5.3/198.5 MB 26.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 6.5/198.5 MB 27.6 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 7.6/198.5 MB 26.9 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 8.7/198.5 MB 27.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 10.0/198.5 MB 26.7 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.1/198.5 MB 28.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 12.2/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 13.3/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 14.3/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 15.3/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 16.4/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 17.4/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 18.7/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 20.0/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 21.4/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 22.7/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 24.1/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 25.3/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 26.8/198.5 MB 27.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 28.1/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 29.0/198.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 30.2/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 31.4/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 32.6/198.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 33.5/198.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 34.3/198.5 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 35.3/198.5 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 36.5/198.5 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 37.2/198.5 MB 23.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 37.8/198.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 38.7/198.5 MB 21.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 40.0/198.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 41.1/198.5 MB 21.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 42.1/198.5 MB 21.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 43.0/198.5 MB 21.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 44.3/198.5 MB 22.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 45.5/198.5 MB 23.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 46.8/198.5 MB 23.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 47.8/198.5 MB 25.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.3/198.5 MB 24.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 48.7/198.5 MB 21.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 49.1/198.5 MB 20.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 50.2/198.5 MB 19.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 51.2/198.5 MB 19.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 52.6/198.5 MB 19.8 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 53.6/198.5 MB 20.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 54.6/198.5 MB 19.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 55.9/198.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 56.9/198.5 MB 19.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 57.9/198.5 MB 19.9 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 59.1/198.5 MB 24.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 60.3/198.5 MB 25.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 61.5/198.5 MB 25.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 62.8/198.5 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 63.8/198.5 MB 24.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 65.4/198.5 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 66.3/198.5 MB 24.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 67.5/198.5 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 68.6/198.5 MB 24.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 69.7/198.5 MB 24.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 71.1/198.5 MB 25.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 72.4/198.5 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 73.9/198.5 MB 25.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 75.2/198.5 MB 26.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 76.7/198.5 MB 26.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 77.9/198.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 79.5/198.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 80.7/198.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 80.7/198.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 80.8/198.5 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 81.6/198.5 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 82.8/198.5 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 84.2/198.5 MB 22.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 85.5/198.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 86.7/198.5 MB 21.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 87.3/198.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 88.1/198.5 MB 19.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 88.6/198.5 MB 18.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 89.2/198.5 MB 18.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 89.7/198.5 MB 17.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 90.2/198.5 MB 16.8 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 90.9/198.5 MB 16.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 92.0/198.5 MB 17.7 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 93.4/198.5 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 94.4/198.5 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 95.5/198.5 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 96.7/198.5 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 98.3/198.5 MB 19.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 99.6/198.5 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 101.2/198.5 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 101.8/198.5 MB 26.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 102.8/198.5 MB 26.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 104.0/198.5 MB 27.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 105.3/198.5 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 106.8/198.5 MB 25.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 108.1/198.5 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 109.4/198.5 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 110.5/198.5 MB 25.1 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.8/198.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 112.7/198.5 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 114.1/198.5 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 115.4/198.5 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 117.0/198.5 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 118.0/198.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 119.3/198.5 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 120.4/198.5 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 121.6/198.5 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 123.1/198.5 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 124.2/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 125.2/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.6/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.0/198.5 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 129.0/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 130.0/198.5 MB 25.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 131.1/198.5 MB 25.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 132.5/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 133.7/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 134.8/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 135.9/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 137.3/198.5 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 138.2/198.5 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 138.9/198.5 MB 24.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 139.6/198.5 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 140.6/198.5 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 141.7/198.5 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 142.5/198.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 142.9/198.5 MB 21.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 143.6/198.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 144.8/198.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 146.1/198.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 147.5/198.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 148.4/198.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 149.5/198.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 151.0/198.5 MB 23.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 152.2/198.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 152.9/198.5 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 153.9/198.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 155.1/198.5 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.6/198.5 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 157.6/198.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.7/198.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 159.8/198.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 160.8/198.5 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.2/198.5 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 163.3/198.5 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 164.5/198.5 MB 28.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 165.9/198.5 MB 28.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.0/198.5 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 168.3/198.5 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 170.0/198.5 MB 28.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 171.2/198.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 172.6/198.5 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 173.7/198.5 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 175.0/198.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 176.6/198.5 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.9/198.5 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.0/198.5 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.9/198.5 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.7/198.5 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.4/198.5 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.7/198.5 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.0/198.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 183.9/198.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.2/198.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 186.7/198.5 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.0/198.5 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 188.8/198.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 190.1/198.5 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.4/198.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.9/198.5 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  193.4/198.5 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  194.4/198.5 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.8/198.5 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.1/198.5 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.5/198.5 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.5/198.5 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.5/198.5 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.5/198.5 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.5/198.5 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 198.5/198.5 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 170.9/170.9 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.2/133.2 kB ? eta 0:00:00\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.9/1.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 15.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.7/5.7 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.6/5.7 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.4/5.7 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.3/5.7 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.4/5.7 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.7/5.7 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 20.4 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.2.1 typing-extensions-4.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\acsl\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.Tensor([[0,0.5], [0.2,0], [0.3, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2000, 0.3000],\n",
       "        [0.5000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2000, 0.3000],\n",
       "        [0.5000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.mm(h, h.t().contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.  , 0.5 ],\n",
       "       [0.  , 0.04, 0.06],\n",
       "       [0.5 , 0.06, 1.09]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "print(np.clip(A.detach().cpu().numpy()[0,1], 0., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.detach().cpu().numpy()[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch==2.2.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch==2.2.1->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.1-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.2 MB 660.6 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.5/1.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\acsl\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR \n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimCLR, self).__init__()\n",
    "\n",
    "    # Encoder \n",
    "    self.f = []\n",
    "    for name, module in resnet50().named_children():\n",
    "      if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d) and not isinstance(module, nn.AdaptiveAvgPool2d):\n",
    "        self.f.append(module)\n",
    "    self.f = nn.Sequential(*self.f)\n",
    "\n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    # Projection head \n",
    "    self.g = nn.Sequential(nn.Linear(2048, 512, bias=False),\n",
    "                           nn.BatchNorm1d(512),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.Linear(512, 128))\n",
    "  def forward(self, x):\n",
    "    x = self.f(x)\n",
    "    x = self.avg_pool(x)\n",
    "    h = torch.flatten(x, start_dim = 1)\n",
    "    z = self.g(h)\n",
    "\n",
    "    return F.normalize(h, dim=-1), F.normalize(z, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConMatching(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config = config \n",
    "\n",
    "    self.backbone = backbone(config)\n",
    "    self.coarse_matching = CorrNet(config['coarse'])\n",
    "    # add the coarse matching process\n",
    "    self.fine_preprocess = FinePreProcessing(config)\n",
    "    self.loftr_fine = LocalFeatureTransformer(config['fine'])\n",
    "    self.fine_matching = FineMatching()\n",
    "\n",
    "# 어떻게 forward할 건지. 원래대로 end-to-end로 ? 아님 다 따로따로 ? \n",
    "  def forward(self, data):\n",
    "    '''\n",
    "    define the data format (?)\n",
    "    '''\n",
    "    # 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from timm) (2.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from timm) (0.17.1)\n",
      "Collecting pyyaml (from timm)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.4.2-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface_hub->timm) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub->timm)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.6 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 751.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface_hub->timm) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision->timm) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 1.4 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.2 MB 1.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.2 MB 1.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 876.1 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.2 MB 1.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.2 MB 1.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.7/2.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 92.2/346.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 194.6/346.4 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 297.0/346.4 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 346.4/346.4 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 92.2/138.7 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 138.7/138.7 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp312-none-win_amd64.whl (270 kB)\n",
      "   ---------------------------------------- 0.0/270.7 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/270.7 kB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/270.7 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 270.7/270.7 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, pyyaml, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.21.4 pyyaml-6.0.1 safetensors-0.4.2 timm-0.9.16 tqdm-4.66.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\acsl\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acsl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm.models.layers import trunc_normal_, DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  '''\n",
    "  ConvNext Block. \n",
    "  (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "  (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back \n",
    "  We use (2) as we find it slightly faster in PyTorch. \n",
    "  refer to TopicFM \n",
    "\n",
    "  Args:\n",
    "    dim (int): Number of input channels.\n",
    "    drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "    layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, kernel_size=3, padding=1, res_op=True):\n",
    "    super().__init__()\n",
    "    self.dwconv = nn.Conv2d(dim, dim, kernel_size=3, padding=padding, groups=dim) #depthwise conv \n",
    "    self.norm = LayerNorm(dim, eps=1e-6)\n",
    "    self.pwconv1 = nn.Linear(dim, 2*dim) # pointwise/1x1 convs, implemented with linear layers \n",
    "    self.act = nn.GELU()\n",
    "    self.pwconv2 = nn.Linear(2*dim, dim)\n",
    "    self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n",
    "                              requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "    self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "    self.res_op = res_op \n",
    "\n",
    "  def forward(self, x):\n",
    "    input = x\n",
    "    x = self.dwconv(x)\n",
    "    x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "    x = self.norm(x)\n",
    "    x = self.pwconv1(x)\n",
    "    x = self.act(x)\n",
    "    x = self.pwconv2(x)\n",
    "    if self.gamma is not None:\n",
    "      x = self.gamma * x\n",
    "    x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "    x = input + self.drop_path(x) if self.res_op else self.drop_path(x)\n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  \"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "  The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "  shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "  with shape (batch_size, channels, height, width).\n",
    "  \"\"\"\n",
    "  def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "      super().__init__()\n",
    "      self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "      self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "      self.eps = eps\n",
    "      self.data_format = data_format\n",
    "      if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "          raise NotImplementedError \n",
    "      self.normalized_shape = (normalized_shape, )\n",
    "\n",
    "  def forward(self, x):\n",
    "      if self.data_format == \"channels_last\":\n",
    "          return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "      elif self.data_format == \"channels_first\":\n",
    "          u = x.mean(1, keepdim=True)\n",
    "          s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "          x = (x - u) / torch.sqrt(s + self.eps)\n",
    "          x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt+FPN, output resolution are 1/8 and 1/2.\n",
    "    Each block has 2 layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Config\n",
    "        block = Block\n",
    "        initial_dim = 128  #config['initial_dim']\n",
    "        block_dims = [128, 192, 256, 384] #config['block_dims']\n",
    "\n",
    "        # Class Variable\n",
    "        self.in_planes = initial_dim\n",
    "\n",
    "        # Networks\n",
    "        self.conv1 = nn.Conv2d(1, initial_dim, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = LayerNorm(initial_dim, eps=1e-6, data_format=\"channels_first\")\n",
    "\n",
    "        self.layer1 = block(block_dims[0], drop_path=0., layer_scale_init_value=0)  # 1/2\n",
    "        self.layer2 = self._make_layer(block, block_dims[1], stride=2)  # 1/4\n",
    "        self.layer3 = self._make_layer(block, block_dims[2], stride=2)  # 1/8\n",
    "        self.layer4 = self._make_layer(block, block_dims[3], stride=2)  # 1/16\n",
    "\n",
    "        # 3. FPN upsample\n",
    "        self.layer3_outconv = nn.Conv2d(block_dims[3], block_dims[2], kernel_size=1, padding=0, bias=False)\n",
    "        self.layer3_outconv2 = nn.Sequential(\n",
    "            block(block_dims[2], drop_path=0., layer_scale_init_value=0),\n",
    "            block(block_dims[2], drop_path=0., layer_scale_init_value=0),\n",
    "            # ConvBlock(block_dims[2], block_dims[2]),\n",
    "            # conv3x3(block_dims[2], block_dims[2]),\n",
    "        )\n",
    "        self.norm_outlayer3 = LayerNorm(block_dims[2], eps=1e-6, data_format=\"channels_first\")\n",
    "        self.layer2_outconv = nn.Conv2d(block_dims[2], block_dims[1], kernel_size=1, padding=0, bias=False)\n",
    "        self.layer2_outconv2 = nn.Sequential(\n",
    "            block(block_dims[1], drop_path=0., layer_scale_init_value=0),\n",
    "            # ConvBlock(block_dims[2], block_dims[1]),\n",
    "            # conv3x3(block_dims[1], block_dims[1]),\n",
    "        )\n",
    "        self.layer1_outconv = nn.Conv2d(block_dims[1], block_dims[0], kernel_size=1, padding=0, bias=False)\n",
    "        self.layer1_outconv2 = nn.Sequential(\n",
    "            block(block_dims[0], drop_path=0., layer_scale_init_value=0),\n",
    "            block(block_dims[0], drop_path=0., layer_scale_init_value=0),\n",
    "            # ConvBlock(block_dims[1], block_dims[0]),\n",
    "            # conv3x3(block_dims[0], block_dims[0]),\n",
    "        )\n",
    "        self.norm_outlayer1 = LayerNorm(block_dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "\n",
    "    def _make_layer(self, block, dim, stride=1):\n",
    "        layer1 = nn.Sequential(nn.Conv2d(self.in_planes, dim, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "                               LayerNorm(dim, eps=1e-6,\n",
    "                                         data_format=\"channels_first\"))  # block(self.in_planes, dim, stride=stride)\n",
    "        layer2 = block(dim, drop_path=0., layer_scale_init_value=0)  # block(dim, dim, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "\n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ResNet Backbone\n",
    "        x0 = self.bn1(self.conv1(x))\n",
    "        x1 = self.layer1(x0)  # 1/2\n",
    "        x2 = self.layer2(x1)  # 1/4\n",
    "        x3 = self.layer3(x2)  # 1/8\n",
    "        x4 = self.layer4(x3)  # 1/16\n",
    "\n",
    "        # FPN\n",
    "        x4_out_2x = F.interpolate(x4, scale_factor=2., mode='bilinear', align_corners=True)\n",
    "        x3_out = self.layer3_outconv2(x3 + self.layer3_outconv(x4_out_2x))\n",
    "\n",
    "        x3_out_2x = F.interpolate(x3_out, scale_factor=2., mode='bilinear', align_corners=True)\n",
    "        x2_out = self.layer2_outconv2(x2 + self.layer2_outconv(x3_out_2x))\n",
    "\n",
    "        x2_out_2x = F.interpolate(x2_out, scale_factor=2., mode='bilinear', align_corners=True)\n",
    "        x1_out = self.layer1_outconv2(x1 + self.layer1_outconv(x2_out_2x))\n",
    "\n",
    "        return [self.norm_outlayer3(x3_out), self.norm_outlayer1(x1_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from opencv-python) (1.26.3)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/38.6 MB 2.6 MB/s eta 0:00:15\n",
      "    --------------------------------------- 0.7/38.6 MB 8.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.2/38.6 MB 9.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.0/38.6 MB 11.6 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.9/38.6 MB 13.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.1/38.6 MB 15.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.0/38.6 MB 16.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.3/38.6 MB 17.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.0/38.6 MB 17.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.4/38.6 MB 16.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 8.6/38.6 MB 17.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 9.9/38.6 MB 18.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.1/38.6 MB 19.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 12.0/38.6 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 12.7/38.6 MB 19.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 13.3/38.6 MB 19.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 14.2/38.6 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 15.1/38.6 MB 19.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 16.5/38.6 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.7/38.6 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 18.9/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 20.0/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 21.3/38.6 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.5/38.6 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 23.7/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.8/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 26.0/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 27.0/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 28.0/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.3/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.6/38.6 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.8/38.6 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.1/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.4/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.8/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/38.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 25.2 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\acsl\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import cv2\n",
    "from types import SimpleNamespace\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "def read_image(path: Path, grayscale: bool = False) -> np.ndarray:\n",
    "    \"\"\"Read an image from path as RGB or grayscale\"\"\"\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"No image at path {path}.\")\n",
    "    mode = cv2.IMREAD_GRAYSCALE if grayscale else cv2.IMREAD_COLOR\n",
    "    image = cv2.imread(str(path), mode)\n",
    "    if image is None:\n",
    "        raise IOError(f\"Could not read image at {path}.\")\n",
    "    if not grayscale:\n",
    "        image = image[..., ::-1]\n",
    "    return image\n",
    "\n",
    "def resize_image(\n",
    "    image: np.ndarray,\n",
    "    size: Union[List[int], int],\n",
    "    fn: str = \"max\",\n",
    "    interp: Optional[str] = \"area\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Resize an image to a fixed size, or according to max or min edge.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    fn = {\"max\": max, \"min\": min}[fn]\n",
    "    if isinstance(size, int):\n",
    "        scale = size / fn(h, w)\n",
    "        h_new, w_new = int(round(h * scale)), int(round(w * scale))\n",
    "        scale = (w_new / w, h_new / h)\n",
    "    elif isinstance(size, (tuple, list)):\n",
    "        h_new, w_new = size\n",
    "        scale = (w_new / w, h_new / h)\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect new size: {size}\")\n",
    "    mode = {\n",
    "        \"linear\": cv2.INTER_LINEAR,\n",
    "        \"cubic\": cv2.INTER_CUBIC,\n",
    "        \"nearest\": cv2.INTER_NEAREST,\n",
    "        \"area\": cv2.INTER_AREA,\n",
    "    }[interp]\n",
    "    return cv2.resize(image, (w_new, h_new), interpolation=mode), scale\n",
    "\n",
    "def numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Normalize the image tensor and reorder the dimensions.\"\"\"\n",
    "    if image.ndim == 3:\n",
    "        image = image.transpose((2, 0, 1))  # HxWxC to CxHxW\n",
    "    elif image.ndim == 2:\n",
    "        image = image[None]  # add channel axis\n",
    "    else:\n",
    "        raise ValueError(f\"Not an image: {image.shape}\")\n",
    "    return torch.tensor(image / 255.0, dtype=torch.float)\n",
    "\n",
    "def load_image(path: Path, resize: int = None, **kwargs) -> torch.Tensor:\n",
    "    image = read_image(path)\n",
    "    if resize is not None:\n",
    "        image, _ = resize_image(image, resize, **kwargs)\n",
    "    return numpy_image_to_torch(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image0 = load_image(\"DSC_0411.JPG\", resize=(640, 480)).unsqueeze(1)\n",
    "image1 = load_image(\"DSC_0410.JPG\", resize=(640,480)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ConvNeXtFPN()\n",
    "feat_c0, feat_f0 = backbone(image0)\n",
    "feat_c1, feat_f1 = backbone(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrNet(nn.Module):\n",
    "  def __init__(self, dim, pre_trained_encoder = None):\n",
    "    super(CorrNet, self).__init__()\n",
    "\n",
    "    # Encoder \n",
    "    if pre_trained_encoder is None:\n",
    "      self.f = []\n",
    "      for name, module in resnet50().named_children():\n",
    "        if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d) and not isinstance(module, nn.AdaptiveAvgPool2d):\n",
    "          self.f.append(module)\n",
    "      self.f = nn.Sequential(*self.f)\n",
    "    else:\n",
    "      pre_trained_encoder_corrnet = SimCLR()\n",
    "      pre_trained_encoder_corrnet.load_state_dict(torch.load(pre_trained_encoder))\n",
    "      self.f = pre_trained_encoder_corrnet.f\n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    self.g = nn.Sequential(nn.Linear(2048, 2048),\n",
    "                           nn.BatchNorm1d(2048),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.Linear(2048, 512),\n",
    "                           nn.BatchNorm1d(512),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.Linear(512, dim))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.f(x)\n",
    "    x = self.avg_pool(x)\n",
    "    h = torch.flatten(x, start_dim =1)\n",
    "    z = self.g(h)\n",
    "\n",
    "    return F.normalize(h, dim=-1), F.normalize(z, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrnet = CorrNet(dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.6 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/44.6 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 20.5/44.6 kB 330.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.0/44.6 kB 326.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.6/44.6 kB 274.9 kB/s eta 0:00:00\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\acsl\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.einops import rearrange\n",
    "feat_c0 = rearrange(feat_c0, 'n c h w -> c n h w')\n",
    "feat_c1 = rearrange(feat_c1, 'n c h w -> c n h w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.concat([feat_c0, feat_c1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 3, 80, 60])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, _ = corrnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2048])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "han_id = torch.argmax(torch.mul(h[0], h[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1503)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "han_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = np.clip(torch.mm(h, h.t().contiguous()).detach().cpu().numpy()[0, 1], 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.937189519405365"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_c0 = feat_c0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_c1 = feat_c1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gradients(x):\n",
    "    x_norm = (x - np.mean(x)) / (np.std(x) + 1e-5)\n",
    "    x_norm = np.swapaxes(np.swapaxes(x_norm, 0, 1), 1, 2)\n",
    "    x_norm = np.max(np.abs(x_norm), axis=2)\n",
    "    x_norm /= np.max(x_norm)\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.22.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-image) (1.26.3)\n",
      "Collecting scipy>=1.8 (from scikit-image)\n",
      "  Downloading scipy-1.12.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 60.4/60.4 kB 796.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-image) (10.2.0)\n",
      "Collecting imageio>=2.27 (from scikit-image)\n",
      "  Downloading imageio-2.34.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2024.2.12-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\acsl\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-image) (23.2)\n",
      "Collecting lazy_loader>=0.3 (from scikit-image)\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading scikit_image-0.22.0-cp312-cp312-win_amd64.whl (25.0 MB)\n",
      "   ---------------------------------------- 0.0/25.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.0 MB 1.3 MB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/25.0 MB 2.2 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 0.3/25.0 MB 2.7 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.6/25.0 MB 3.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.8/25.0 MB 3.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 1.0/25.0 MB 4.2 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.1/25.0 MB 3.5 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 1.1/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.2/25.0 MB 3.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.4/25.0 MB 3.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.6/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.7/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.8/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.0/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.2/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.3/25.0 MB 3.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.4/25.0 MB 3.1 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.5/25.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.5/25.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.7/25.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.8/25.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.0/25.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 3.1/25.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 3.4/25.0 MB 3.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.6/25.0 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.8/25.0 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.9/25.0 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.0/25.0 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.1/25.0 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.2/25.0 MB 3.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.2/25.0 MB 3.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.4/25.0 MB 3.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.4/25.0 MB 3.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.5/25.0 MB 2.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 4.6/25.0 MB 2.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 4.7/25.0 MB 2.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 5.0/25.0 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.3/25.0 MB 3.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 5.7/25.0 MB 3.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.0/25.0 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.3/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.3/25.0 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.7/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.8/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.0/25.0 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.1/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.4/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.5/25.0 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.5/25.0 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.7/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.9/25.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 8.1/25.0 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.6/25.0 MB 3.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.0/25.0 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.5/25.0 MB 3.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.0/25.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.5/25.0 MB 4.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 11.1/25.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.7/25.0 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.3/25.0 MB 4.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.8/25.0 MB 5.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.4/25.0 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.2/25.0 MB 6.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.9/25.0 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.7/25.0 MB 8.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.5/25.0 MB 8.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.4/25.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.3/25.0 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.0/25.0 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.0/25.0 MB 15.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.9/25.0 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.8/25.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.8/25.0 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/25.0 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.0 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.0/25.0 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.0/25.0 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading imageio-2.34.0-py3-none-any.whl (313 kB)\n",
      "   ---------------------------------------- 0.0/313.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 313.4/313.4 kB 20.2 MB/s eta 0:00:00\n",
      "Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Downloading scipy-1.12.0-cp312-cp312-win_amd64.whl (45.8 MB)\n",
      "   ---------------------------------------- 0.0/45.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.2/45.8 MB 24.8 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 2.1/45.8 MB 22.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 2.8/45.8 MB 22.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 3.8/45.8 MB 22.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 4.3/45.8 MB 19.7 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 5.2/45.8 MB 19.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 6.3/45.8 MB 20.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 7.3/45.8 MB 20.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.4/45.8 MB 20.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.3/45.8 MB 20.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.6/45.8 MB 21.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.6/45.8 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 12.7/45.8 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.7/45.8 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.8/45.8 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.9/45.8 MB 23.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 17.0/45.8 MB 23.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 18.2/45.8 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 19.4/45.8 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 20.7/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 21.8/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 23.1/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 24.2/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 25.4/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.4/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 27.5/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 28.8/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 29.9/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.3/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.5/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.7/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 34.9/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.0/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.4/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.7/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 40.0/45.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.4/45.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 42.6/45.8 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.1/45.8 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.9/45.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.7/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.8/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.8/45.8 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading tifffile-2024.2.12-py3-none-any.whl (224 kB)\n",
      "   ---------------------------------------- 0.0/224.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 224.5/224.5 kB 14.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tifffile, scipy, lazy_loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.34.0 lazy_loader-0.3 scikit-image-0.22.0 scipy-1.12.0 tifffile-2024.2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\acsl\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_maps(input_image1, input_image2, corrnet, resize_to_input=True):\n",
    "    \"\"\"\n",
    "\n",
    "    This function returns activation maps for each of the 2 images. Each activation map highlights regions which\n",
    "    seem similar to the other images content.\n",
    "\n",
    "    It is using the feature tensor f before the average pooling and the image descriptor h directly after the\n",
    "    average pooling. Given f1, h1 and f2,\n",
    "\n",
    "    Arguments:\n",
    "        input_image1 (torch.Tensor): Reference image in the shape (1, 3, height, width)\n",
    "        input_image2 (torch.Tensor): Target image in the shape (1, 3, height, width). Shape can be\n",
    "            different from input_image1\n",
    "        corrnet (CorrNet): Instance of CorrNet.\n",
    "        resize_to_input (bool): If true, the returned activation map will have the same width and height as the input\n",
    "            images. If False, the activation maps will have the width and height of each image's feature map before\n",
    "            the average pooling is applied\n",
    "\n",
    "    Returns:\n",
    "        Tuple of two activation maps (reference_map, target_map). Each map is a numpy array in the shape of (height, width)\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # get features\n",
    "        f1 = corrnet.f(input_image1)\n",
    "        f2 = corrnet.f(input_image2)\n",
    "        _, channels, height1, width1 = f1.shape\n",
    "        _, channels, height2, width2 = f2.shape\n",
    "\n",
    "        # flatten f for multiplication later\n",
    "        f1_flat = f1.reshape(channels, height1 * width1)\n",
    "        f2_flat = f2.reshape(channels, height2 * width2)\n",
    "\n",
    "        # get h\n",
    "        h1 = F.normalize(torch.mean(f1_flat, dim=1), dim=0)\n",
    "        h2 = F.normalize(torch.mean(f2_flat, dim=1), dim=0)\n",
    "\n",
    "        # multiply each cell in f from one image with h of the other image\n",
    "        f1_activation = torch.mean((f1_flat * h2.reshape(channels, 1)).reshape(channels, height1, width1), dim=0)\n",
    "        f2_activation = torch.mean((f2_flat * h1.reshape(channels, 1)).reshape(channels, height2, width2), dim=0)\n",
    "\n",
    "        # normalization the activations to a range of [0, 1]\n",
    "        f1_activation_normalized = f1_activation - torch.min(f1_activation)\n",
    "        f2_activation_normalized = f2_activation - torch.min(f2_activation)\n",
    "        f1_activation_normalized /= torch.max(f1_activation_normalized)\n",
    "        f2_activation_normalized /= torch.max(f2_activation_normalized)\n",
    "\n",
    "        # get numpy\n",
    "        activation_map1 = f1_activation.cpu().data.numpy()\n",
    "        activation_map2 = f2_activation.cpu().data.numpy()\n",
    "\n",
    "        if resize_to_input:\n",
    "            # resize the activation map to the image size and apply a gaussian kernel\n",
    "            activation_map1 = resize(activation_map1, (input_image1.shape[2], input_image2.shape[3]))\n",
    "            activation_map2 = resize(activation_map2, (input_image2.shape[2], input_image2.shape[3]))\n",
    "            activation_map1 = gaussian_filter(activation_map1, sigma=min(input_image1.shape[2] // height1, input_image1.shape[3] // width1))\n",
    "            activation_map2 = gaussian_filter(activation_map2, sigma=min(input_image2.shape[2] // height2, input_image2.shape[3] // width2))\n",
    "\n",
    "    return activation_map1, activation_map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints(input_image1, input_image2, corrnet, guided_backprop, output_neuron_idx, hysteresis_threshold=(0.0, 0.0), nms=(3,3), top_k = 2000):\n",
    "    '''\n",
    "    Arguments:\n",
    "        input_image1 (torch.Tensor) : (batch, 3, height, width)\n",
    "        input_image2 (torch.Tensor) : (batch, 3, height, width)\n",
    "        corrnet (CorrNet): CorrNet instance \n",
    "        guided_backprop: Guided backpropagation instance of corrnet \n",
    "        output_neuron_idx: index of the highest activated neuron of h \n",
    "        hysteresis_threshold: ... \n",
    "        nms: ... \n",
    "        top_k: Take k best keypoints of each image \n",
    "\n",
    "    Returns:\n",
    "        The keypoints and gradient maps in the following format:\n",
    "            [(keypoints1, grads1), (keypoints2, grads2)]\n",
    "        Keypoints are in the shape (number_of_keypoints, 3) with the values being [y, x, likelihood]\n",
    "        Gradients are in the shape (height, width) and have the same resolution as their corresponding input images \n",
    "\n",
    "    '''\n",
    "\n",
    "    # Get gradients \n",
    "    grads1 = guided_backprop(input_image1, output_neuron_idx)\n",
    "    grads2 = guided_backprop(input_image2, output_neuron_idx)\n",
    "    grads_np1 = grads1.cpu().data.numpy()\n",
    "    grads_np2 = grads2.cpu().data.numpy()\n",
    "\n",
    "    # Multiply the gradients with the activation map if wanted \n",
    "    activation1, activation2 = get_activation_maps(input_image1, input_image2, corrnet)\n",
    "    grads_np1 *= np.expand_dims(activation1, 0)\n",
    "    grads_np2 *= np.expand_dims(activation2, 0)\n",
    "\n",
    "    # Normalize the grdients \n",
    "    grads_np1 = normalize_gradients(grads_np1)\n",
    "    grads_np2 = normalize_gradients(grads_np2)\n",
    "\n",
    "    # Apply post processing and extract keypoints from each gradient map \n",
    "    out = []\n",
    "    for grads_np, activations_np in [(grads_np1, activation1), (grads_np2, activation2)]:\n",
    "        # NMS from Kornia\n",
    "        if nms[0] > 0:\n",
    "            grads_pth = torch.tensor(grads_np)\n",
    "            grads_pth = grads_pth.unsqueeze(0).unsqueeze(0)\n",
    "            grads_pth = feature.non_maxima_suppression2d(grads_pth, (nms[0], nms[1]))\n",
    "\n",
    "            # Convert to numpy\n",
    "            grads = grads_pth[0][0].cpu().data.numpy()\n",
    "        else:\n",
    "            grads = grads_np\n",
    "\n",
    "        # Hysteresis threshold\n",
    "        hysteresis_threshold_min, hysteresis_threshold_max = hysteresis_threshold\n",
    "        grads_binary = apply_hysteresis_threshold(grads, hysteresis_threshold_min, hysteresis_threshold_max)\n",
    "\n",
    "        # Keypoints\n",
    "        keypoints = np.swapaxes(np.vstack(np.nonzero(grads_binary)), 0, 1)\n",
    "        # Get likelihood based on gradients\n",
    "        keypoints_likelihood = np.array([grads[k[0]][k[1]] for k in keypoints])\n",
    "\n",
    "        # Get top keypoints based on the likelihoods\n",
    "        if top_k > 0.0:\n",
    "            idx_sort = np.argsort(keypoints_likelihood)[-top_k:]\n",
    "            keypoints = keypoints[idx_sort]\n",
    "            keypoints_likelihood = keypoints_likelihood[idx_sort]\n",
    "\n",
    "        out.append(np.hstack((keypoints, keypoints_likelihood.reshape(-1, 1))))\n",
    "\n",
    "    return out        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class GuidedBackpropReLU(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
    "        ctx.save_for_backward(input, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_saved_tensors, output = ctx.saved_tensors\n",
    "\n",
    "        positive_mask_1 = (input_saved_tensors > 0).type_as(grad_output)\n",
    "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
    "\n",
    "        grad_input = torch.addcmul(torch.zeros(input_saved_tensors.size()).type_as(input_saved_tensors),\n",
    "                                   torch.addcmul(torch.zeros(input_saved_tensors.size()).type_as(input_saved_tensors), grad_output, positive_mask_1),\n",
    "                                   positive_mask_2)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class GuidedBackpropReLUModel:\n",
    "    def __init__(self, network, device):\n",
    "        self.network = network\n",
    "        self.network = network.to(device)\n",
    "        self.network.eval()\n",
    "        self.device = device\n",
    "\n",
    "        def recursive_relu_apply(modules):\n",
    "            for idx, module in modules._modules.items():\n",
    "                recursive_relu_apply(module)\n",
    "                if module.__class__.__name__ == 'ReLU':\n",
    "                    modules._modules[idx] = GuidedBackpropReLU.apply\n",
    "\n",
    "        recursive_relu_apply(self.network)\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        h, z = self.network(input_image)\n",
    "        return h\n",
    "\n",
    "    def __call__(self, input_image, idx=None):\n",
    "        # Predict input image\n",
    "        output = self.forward(input_image)\n",
    "\n",
    "        # Initialize one hot output mask\n",
    "        if idx is None:\n",
    "            idx = torch.argmax(output)\n",
    "        one_hot = torch.zeros(output.size())\n",
    "        one_hot[0][idx] = 1\n",
    "        one_hot = one_hot.to(self.device)\n",
    "        one_hot.requires_grad = True\n",
    "\n",
    "        # Apply mask and back-propagate the activation to the input space\n",
    "        one_hot = torch.neg(torch.sum(one_hot * output))\n",
    "        one_hot.backward()\n",
    "\n",
    "        # Return the input gradients w.r.t. the output neuron\n",
    "        return input_image.grad[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_bp = GuidedBackpropReLUModel(corrnet, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grads1 \u001b[38;5;241m=\u001b[39m \u001b[43mguided_bp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_c0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhan_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 59\u001b[0m, in \u001b[0;36mGuidedBackpropReLUModel.__call__\u001b[1;34m(self, input_image, idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Apply mask and back-propagate the activation to the input space\u001b[39;00m\n\u001b[0;32m     58\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mneg(torch\u001b[38;5;241m.\u001b[39msum(one_hot \u001b[38;5;241m*\u001b[39m output))\n\u001b[1;32m---> 59\u001b[0m \u001b[43mone_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Return the input gradients w.r.t. the output neuron\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_image\u001b[38;5;241m.\u001b[39mgrad[\u001b[38;5;241m0\u001b[39m, :, :, :]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "grads1 = guided_bp(feat_c0, han_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acsl\\AppData\\Local\\Temp\\ipykernel_5644\\2414449266.py:62: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  return input_image.grad[0, :, :, :]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m list_keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_c0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_c1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguided_bp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 22\u001b[0m, in \u001b[0;36mdetect_keypoints\u001b[1;34m(input_image1, input_image2, corrnet, guided_backprop, output_neuron_idx, hysteresis_threshold, nms, top_k)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    input_image1 (torch.Tensor) : (batch, 3, height, width)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get gradients \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m grads1 \u001b[38;5;241m=\u001b[39m \u001b[43mguided_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_neuron_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m grads2 \u001b[38;5;241m=\u001b[39m guided_backprop(input_image2, output_neuron_idx)\n\u001b[0;32m     24\u001b[0m grads_np1 \u001b[38;5;241m=\u001b[39m grads1\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[51], line 62\u001b[0m, in \u001b[0;36mGuidedBackpropReLUModel.__call__\u001b[1;34m(self, input_image, idx)\u001b[0m\n\u001b[0;32m     59\u001b[0m one_hot\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Return the input gradients w.r.t. the output neuron\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minput_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "list_keypoints = detect_keypoints(feat_c0, feat_c1, corrnet, guided_bp, han_id, top_k=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
